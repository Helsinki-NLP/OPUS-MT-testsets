# OPUS-MT-eval - benchmarks for evaluating MT models

A collection of machine translation benchmarks. Currently it includes:

* [WMT news translation](http://www.statmt.org/wmt20/translation-task.html)
* [Tico19 tanslation benchmark](https://tico-19.github.io)
* [Test sets from the Tatoeba translation challenge](https://github.com/Helsinki-NLP/Tatoeba-Challenge/)

