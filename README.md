# OPUS-MT-eval - benchmarks for evaluating MT models

A collection of machine translation benchmarks. Currently it includes:

* [WMT news translation](http://www.statmt.org/wmt21/translation-task.html)
* [Tico19 tanslation benchmark](https://tico-19.github.io)
* [Test sets from the Tatoeba translation challenge](https://github.com/Helsinki-NLP/Tatoeba-Challenge/)
* [FLORES1 data sets](https://github.com/facebookresearch/flores/)
* [FLORES-101 dev and devtest](https://github.com/facebookresearch/flores/)


## License

* TICO-19 is made publicly available through a [Creative Commons CC0](LICENSE-CC0.md) license.
* FLORES-101 is licenced under [CC-BY-SA](LICENSE-CC-BY-SA)
